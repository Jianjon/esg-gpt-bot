import streamlit as st
st.set_page_config(page_title="ESG æ·¨é›¶å°å¹«æ‰‹", page_icon="ğŸŒ±", layout="centered")

from src.welcome import show_welcome_page

if "user_name" not in st.session_state or not st.session_state.get("intro_survey_submitted"):
    show_welcome_page()
    st.stop()

if "intro_survey_submitted" not in st.session_state:
    st.warning("âš ï¸ è«‹å…ˆå®Œæˆ ESG å‰å°å•å·ï¼ˆå·²æ•´åˆæ–¼ Welcome é é¢ï¼‰")
    st.stop()

# _init_app ç¸½é«”è™•ç†ç’°å¢ƒè¨­å®šèˆ‡ sys.path
import _init_app

# å¥—ç”¨è‡ªå®šç¾© CSS æ¨£å¼
with open("assets/custom_style.css", encoding="utf-8") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# é¡¯ç¤º Topbar
st.markdown("""
    <style>
        div[data-testid="stAppViewContainer"] > .main {{
            padding-top: 64px;
        }}
    </style>
    <div class="topbar">
        <div class="logo">ğŸŒ± ESG Service Path</div>
        <div class="user-info">{}</div>
    </div>
""".format(st.session_state.get("user_name", "æœªç™»å…¥").upper()), unsafe_allow_html=True)

from collections import defaultdict
from pathlib import Path
import pandas as pd
import os
import json
import matplotlib.pyplot as plt
import faiss
import fitz
import re
from typing import List, Dict, Tuple
from src.utils.prompt_builder import generate_user_friendly_prompt  # æ”¾åœ¨é€™æ®µæœ€ä¸Šé¢ï¼ˆåªè¦åŒ¯å…¥ä¸€æ¬¡ï¼‰
from sessions.answer_session import AnswerSession
from sessions.context_tracker import get_conversation, add_context_entry, add_turn
# from sessions.context_tracker import get_all_summaries
from managers.baseline_manager import BaselineManager
from managers.report_manager import ReportManager
from managers.feedback_manager import FeedbackManager
from session_logger import save_to_json, load_from_json, save_to_sqlite
from vector_builder.pdf_processor import PDFProcessor
from vector_builder.metadata_handler import MetadataHandler
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from src.components.suggest_box import render_suggested_questions

MODULE_MAP = {
    "C": "ESG æ•™å­¸å°å…¥ï¼ˆæ•™å­¸å‰å°ï¼‰",
    "B": "é‚Šç•Œè¨­å®šèˆ‡çµ„ç¹”è³‡è¨Š",
    "S": "æ’æ”¾æºè¾¨è­˜èˆ‡ç¢ºèª",
    "D": "æ•¸æ“šæ”¶é›†æ–¹å¼èˆ‡èƒ½åŠ›",
    "M": "å…§éƒ¨ç®¡ç†èˆ‡SOPç¾æ³",
    "R": "å ±å‘Šéœ€æ±‚èˆ‡å¾ŒçºŒè¡Œå‹•"
}

INDUSTRY_FILE_MAP = {
    "é¤é£²æ¥­": "Restaurant.csv",
    "æ—…å®¿æ¥­": "Hotel.csv",
    "é›¶å”®æ¥­": "Retail.csv",
    "å°å‹è£½é€ æ¥­": "SmallManufacturing.csv",
    "ç‰©æµæ¥­": "Logistics.csv",
    "è¾¦å…¬å®¤æœå‹™æ¥­": "Offices.csv"
}

STAGE_MAP = {
    "basic": "beginner",
    "advanced": "intermediate"
}

TOPIC_ORDER = [
    "ESG æ•™å­¸å°å…¥ï¼ˆæ•™å­¸å‰å°ï¼‰",
    "é‚Šç•Œè¨­å®šèˆ‡çµ„ç¹”è³‡è¨Š",
    "æ’æ”¾æºè¾¨è­˜èˆ‡ç¢ºèª",
    "æ•¸æ“šæ”¶é›†æ–¹å¼èˆ‡èƒ½åŠ›",
    "å…§éƒ¨ç®¡ç†èˆ‡SOPç¾æ³",
    "å ±å‘Šéœ€æ±‚èˆ‡å¾ŒçºŒè¡Œå‹•"
]

def load_questions(industry: str, stage: str = "basic", skip_common: bool = False) -> list:
    filename = INDUSTRY_FILE_MAP.get(industry)
    if not filename:
        raise ValueError(f"æ‰¾ä¸åˆ°ç”¢æ¥­å°æ‡‰çš„é¡Œåº«ï¼š{industry}")
    path = os.path.join("data", filename)
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é¡Œåº«æª”æ¡ˆï¼š{path}")

    df = pd.read_csv(path)
    required_cols = ["question_id", "question_text", "difficulty_level", "option_type", "answer_tags"]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"é¡Œåº«ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{col}")

    if stage not in STAGE_MAP:
        raise ValueError(f"è¼¸å…¥å€¼éŒ¯èª¤ï¼šstage æ‡‰ç‚º basic æˆ– advancedï¼Œä½†æ”¶åˆ°ï¼š{stage}")
    mapped_stage = STAGE_MAP[stage]
    df = df[df["difficulty_level"].isin([mapped_stage] if stage == "basic" else ["beginner", "intermediate"])]

    questions = []
    for _, row in df.iterrows():
        qid = row["question_id"]
        topic = row.get("topic_category", "").strip() or MODULE_MAP.get(qid[0], "æœªåˆ†é¡")
        options, option_notes = [], {}
        for opt in ["A", "B", "C", "D", "E"]:
            val = row.get(f"option_{opt}")
            if pd.notna(val):
                options.append(val)
                option_notes[val] = row.get(f"option_{opt}_note", "")

        questions.append({
            "id": qid,
            "industry": row.get("industry_type", industry),
            "text": row.get("question_text", "æœªå¡«é¡Œç›®å…§å®¹"),
            "options": options,
            "option_notes": option_notes,
            "type": row.get("option_type", "single"),
            "topic": topic,
            "difficulty": row.get("difficulty_level", mapped_stage),
            "report_section": row.get("report_section", ""),
            "tags": row.get("answer_tags", "").split("|") if isinstance(row.get("answer_tags"), str) else [],
            "allow_custom_answer": row.get("allow_custom_answer", False),
            "allow_skip": row.get("allow_skip", False),
            "note": row.get("free_answer_note", ""),
            "question_note": row.get("question_note", ""),
            "learning_objective": row.get("learning_objective", ""),
            "report_topic": row.get("report_topic", ""),
            "learning_goal": row.get("learning_goal", ""),
            "follow_up": row.get("follow_up", "")
        })

    if skip_common:
        questions = [q for q in questions if not (q["id"].startswith("C0"))]

    def question_sort_key(q):
        topic_index = TOPIC_ORDER.index(q["topic"]) if q["topic"] in TOPIC_ORDER else 999
        qid = q["id"]
        num_part = int(qid[1:]) if qid[1:].isdigit() else 999
        return (topic_index, num_part)

    return sorted(questions, key=question_sort_key)

# ===== åˆå§‹åŒ–è¨­å®š =====
if "stage" not in st.session_state:
    st.session_state.stage = "basic"
if "jump_to" not in st.session_state:
    st.session_state["jump_to"] = None

user_id = f"{st.session_state.get('company_name', 'unknown')}_{st.session_state.get('user_name', 'user')}"
current_stage = st.session_state.stage

questions = load_questions(
    industry=st.session_state.industry,
    stage=current_stage,
    skip_common=True
)

if "session" not in st.session_state:
    session_file = Path("data/sessions") / f"{st.session_state.company_name}_{st.session_state.user_name}.json"
    if session_file.exists():
        if st.session_state.get("reset_data"):
            session_file.unlink()
            st.toast("âœ… å·²æ¸…é™¤åŸæœ‰ç´€éŒ„ï¼Œå•å·å°‡å¾é ­é–‹å§‹ã€‚")
            st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)
        else:
            session = load_from_json(user_id, questions)
            if session:
                if st.button("ğŸ”„ ç¹¼çºŒä¸Šæ¬¡ç­”é¡Œé€²åº¦"):
                    st.session_state.session = session
                    st.rerun()
                else:
                    st.warning("åµæ¸¬åˆ°æ‚¨æœ‰æœªå®Œæˆçš„å•å·ç´€éŒ„ï¼Œæ‚¨å¯ä»¥é¸æ“‡ç¹¼çºŒæˆ–é‡æ–°é–‹å§‹ã€‚")
                    st.stop()
    else:
        st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)

session = st.session_state.session
current_q = session.get_current_question()

# ===== å•Ÿç”¨ jump_to è·³é¡Œ =====
if st.session_state.get("jump_to"):
    qid = st.session_state["jump_to"]
    index = next((i for i, item in enumerate(session.question_set) if item["id"] == qid), None)
    if index is not None:
        session.jump_to(index)
        st.session_state["jump_to"] = None
        st.rerun()

# ===== å´é‚Šåˆ—é‡æ§‹ï¼ˆä½¿ç”¨ question_setï¼‰ =====
with st.sidebar:
    st.title("ğŸ“‹ ESG Service Path | æ·¨é›¶å°å¹«æ‰‹")
    st.markdown("---")
    st.header("ğŸ‘¤ ä½¿ç”¨è€…è³‡è¨Š")
    st.markdown(f"**å§“åï¼š** {st.session_state.user_name}")
    st.markdown(f"**éšæ®µï¼š** {'åˆéš' if st.session_state.stage == 'basic' else 'é€²éš'}")
    st.markdown(f"**ç›®å‰é€²åº¦ï¼š** {session.current_index + 1} / {len(session.question_set)}")
    st.markdown("---")
    st.markdown("### ğŸ“Š ä¸»é¡Œé€²åº¦æ¦‚è¦½")

    current_topic = current_q.get("topic") if current_q else None
    answered_ids = {r["question_id"] for r in session.responses}

    questions_by_topic = defaultdict(list)
    for q in session.question_set:
        topic = q.get("topic", "æœªåˆ†é¡")
        questions_by_topic[topic].append(q)

    for topic, q_list in questions_by_topic.items():
        total = len(q_list)
        answered = sum(1 for q in q_list if q["id"] in answered_ids)
        checked = "âœ… " if answered == total else ""
        expanded = topic == current_topic

        with st.expander(f"{checked}{topic}", expanded=expanded):
            for idx, q in enumerate(q_list, 1):
                is_done = q["id"] in answered_ids
                label = f"{idx}. {q.get('text', '')[:15]}..."
                if is_done:
                    label += " âœ”"

                key = f"jump_to_{q['id']}"
                if st.button(label, key=key):
                    st.session_state["jump_to"] = q["id"]
                    st.rerun()


# å›ºå®šä¸»é«”å®¹å™¨
st.markdown('<div class="main-content-container">', unsafe_allow_html=True)

# é¡¯ç¤ºé¡§å•å¼•å°
friendly_intro = generate_user_friendly_prompt(current_q, st.session_state.user_intro_survey)

st.markdown("#### ğŸ’¬ é¡§å•å¼•å°")
st.markdown(f"**{friendly_intro}**")  # âœ… é¡¯ç¤ºç²—é«”å¼•å°èª

# ä¸»é«”å…§å®¹ï¼šå•é¡Œä¸»é«”ã€èªªæ˜èˆ‡é¸é …
if current_q:
    # ===== é¡¯ç¤ºå­¸ç¿’ç›®æ¨™ =====
    goal = current_q.get("learning_goal", "")
    st.markdown("#### ğŸ¯ å­¸ç¿’ç›®æ¨™")
    st.markdown(f"<p style='font-size:18px'><strong><em>{goal}</em></strong></p>", unsafe_allow_html=True)

    #===== é¡¯ç¤ºé¡Œç›®ä¸»æ–‡ =====
    st.markdown("---")
    st.markdown("### ğŸ“ é¡Œç›®")
    st.markdown(f"<p style='font-size:18px'><strong>{current_q.get('text', '')}</strong></p>", unsafe_allow_html=True)

    st.markdown("  \n")
    # ===== é¡¯ç¤ºé¡Œç›®èªªæ˜ï¼ˆè‹¥æœ‰ï¼‰=====
    note = current_q.get("question_note", "")
    if note:
        st.markdown("#### ğŸ—’ï¸ é¡Œç›®èªªæ˜")
        st.markdown(f"<div style='font-size:16px'>{note}</div>", unsafe_allow_html=True)

    st.markdown("  \n")

    # é¡¯ç¤ºé¸é …å€‘
    options = current_q["options"]
    option_notes = current_q.get("option_notes", {})

    formatted_options = []
    for opt in options:
        note = option_notes.get(opt, "")
        html = f"<strong>{opt}</strong>ï¼š<span style='font-size:15px'>{note}</span>"
        formatted_options.append(html)

    selected = []

    if current_q["type"] == "single":
        selected_html = st.radio("å¯é¸æ“‡ï¼š", formatted_options, format_func=lambda x: x, index=0, key="radio_options", label_visibility="visible")
        if selected_html:
            selected = [options[formatted_options.index(selected_html)]]
    else:
        st.markdown("å¯è¤‡é¸ï¼š")
        for i, html in enumerate(formatted_options):
            opt_key = options[i]
            if st.checkbox(html, key=opt_key):
                selected.append(opt_key)
    # âœ… è‹¥å…è¨±ä½¿ç”¨è€…è‡ªè¨‚ç­”æ¡ˆ
    custom_input = ""
    if current_q.get("allow_custom_answer", False):
        st.markdown("#### âœï¸ <strong>è‹¥ä»¥ä¸Šé¸é …éƒ½ä¸åˆé©ï¼Œè«‹å¡«å¯«æ‚¨çš„è‡ªè¨‚ç­”æ¡ˆï¼š</strong>", unsafe_allow_html=True)
        custom_input = st.text_input("è«‹è¼¸å…¥æ‚¨çš„æƒ³æ³•æˆ–åšæ³•...", key="custom_input")

import streamlit as st
from src.utils.prompt_builder import build_learning_prompt, generate_user_friendly_prompt
from src.utils.gpt_tools import call_gpt

# å‡è¨­çš„å•é¡Œè³‡æ–™å’Œç”¨æˆ¶è¼¸å…¥
current_q = {"id": "q1", "type": "single", "question": "ä½ çš„å…¬å¸æ˜¯å¦é—œæ³¨ ESG è­°é¡Œï¼Ÿ"}
selected = st.selectbox("é¸æ“‡ç­”æ¡ˆï¼š", ["æ˜¯", "å¦"])
selected = [selected]
custom_input = st.text_input("å¦‚æœæœ‰å…¶ä»–æƒ³æ³•ï¼Œè«‹è¼¸å…¥ï¼š", value="")

# ç¢ºä¿å¿…è¦çš„è®Šæ•¸å·²å®šç¾©
if "user_intro_survey" not in st.session_state:
    st.session_state.user_intro_survey = {"background": "æœªçŸ¥"}

# ç”Ÿæˆä»‹ç´¹æ€§å›æ‡‰
friendly_intro = generate_user_friendly_prompt(current_q, st.session_state.user_intro_survey)
st.write("ä»‹ç´¹ï¼š", friendly_intro)

# å¾ŒçºŒçš„ GPT æ•™å­¸é‚è¼¯
if custom_input:
    selected = [custom_input] if current_q["type"] == "single" else selected + [custom_input]

user_profile = {
    "user_name": st.session_state.get("user_name", ""),
    "company_name": st.session_state.get("company_name", ""),
    "industry": st.session_state.get("industry", ""),
    **st.session_state.get("user_intro_survey", {})
}

# æ§‹å»º prompt
prompt_text = build_learning_prompt(user_profile, current_q, user_answer)

# æŒ‰éˆ•è§¸ç™¼ GPT å›æ‡‰ï¼ˆè£œå……ç¬¬ä¸€æ®µçš„æŒ‰éˆ•ï¼‰
if st.button("ğŸ¤– ç”± ESG å°å¹«æ‰‹ç”¢ç”Ÿæ•™å­¸å¼•å°", key="btn_gpt_teaching"):
    with st.chat_message("assistant"):
        with st.spinner("AI æ•™å­¸ä¸­ï¼Œè«‹ç¨å€™..."):  # ä½¿ç”¨ç¬¬äºŒæ®µçš„æç¤ºè¨Šæ¯
            try:
                gpt_reply = call_gpt(prompt_text, temperature=0.7)
                st.markdown(gpt_reply)
                add_turn(current_q["id"], prompt_text, gpt_reply)
            except Exception as e:
                st.error(f"âš ï¸ GPT å›è¦†å¤±æ•—ï¼š{str(e)}")

    # å°èˆªæŒ‰éˆ•å€å¡Š
    st.markdown("---")
    st.markdown("#### ğŸ§­ è«‹ç¢ºèªæ‚¨çš„ä½œç­”ï¼Œç„¶å¾Œé»æ“Šã€Œä¸‹ä¸€é¡Œã€æˆ–è¿”å›ä¿®æ”¹ï¼š")
    col1, col2 = st.columns([1, 1])

    with col1:
        if st.button("â¬…ï¸ ä¸Šä¸€é¡Œ", key="btn_prev", use_container_width=True):
            session.go_back()
            st.rerun()

    with col2:
        if st.button("â¡ï¸ ä¸‹ä¸€é¡Œï¼ˆæäº¤ç­”æ¡ˆï¼‰", key="btn_next", use_container_width=True):
            if current_q["type"] == "single":
                answer_payload = selected[0] if selected else ""
            else:
                answer_payload = selected

            if custom_input:
                answer_payload = [custom_input] if current_q["type"] == "single" else selected + [custom_input]

            if not answer_payload:
                st.warning("âš ï¸ è«‹å…ˆé¸æ“‡æˆ–å¡«å¯«ä¸€å€‹ç­”æ¡ˆå¾Œå†ç¹¼çºŒ")
                st.stop()

            result = session.submit_response(answer_payload)
            add_context_entry(current_q["id"], answer_payload, current_q["text"])
            save_to_json(session)

            if "error" in result:
                st.error(result["error"])
            else:
                st.success("âœ… å·²æäº¤ï¼Œå³å°‡è·³è½‰è‡³ä¸‹ä¸€é¡Œ")
                st.rerun()

    st.markdown("  \n")
    # === GPT å°è©±å¼å•ç­”å€å¡Š ===
    st.divider()
    st.markdown("#### ğŸ¤– æ·¨é›¶å°å¹«æ‰‹ï¼ˆæ¸¬è©¦éšæ®µä»¥äº”é¡Œç‚ºé™ï¼‰")

    # åˆå§‹åŒ–å°è©±è¨˜æ†¶
    if "qa_threads" not in st.session_state:
        st.session_state.qa_threads = {}

    chat_id = current_q["id"]
    from sessions.context_tracker import get_conversation, add_turn
    from src.utils.gpt_tools import call_gpt

    # é¡¯ç¤ºæ­·å²å°è©±
    for msg in get_conversation(chat_id):
        if "user" in msg:
            with st.chat_message("user"):
                st.markdown(msg["user"])
        if "gpt" in msg:
            with st.chat_message("assistant"):
                st.markdown(msg["gpt"])



    # å®šç¾©é»æŒ‰æŒ‰éˆ•å¾Œè‡ªå‹•é€å‡ºçš„è™•ç†æµç¨‹
    def auto_submit_prompt(selected_prompt):
        from src.utils.gpt_tools import call_gpt
        from sessions.context_tracker import add_turn, get_conversation

        with st.chat_message("user"):
            st.markdown(selected_prompt)
        with st.chat_message("assistant"):
            with st.spinner("AI å›è¦†ä¸­..."):
                context_text = f"{current_q['text']}\n{current_q.get('learning_goal', '')}"
                reply = call_gpt(
                prompt=selected_prompt,
                question_text=current_q["text"],
                learning_goal=current_q.get("learning_goal", ""),
                chat_history=get_conversation(current_q["id"]),
                industry=st.session_state.get("industry", "")
            )

                st.markdown(reply)
                add_turn(current_q["id"], selected_prompt, reply)

    # ğŸ’¡ è‡ªå‹•å¾é¡Œç›®å¸¶å…¥å»ºè­°æå•ï¼ˆæ”¯æ´è‡ªç„¶èªå¥æ ¼å¼ï¼‰
    suggested_prompts_raw = current_q.get("follow_up", "")
    suggested_prompts = [
        s.strip() + "ï¼Ÿ"
        for s in suggested_prompts_raw.split("ï¼Ÿ")
        if s.strip()
]

    # å»ºè­°æå•æŒ‰éˆ•å€
    st.markdown("#### ğŸ’¬ æƒ³æ·±å…¥äº†è§£ï¼Ÿå¯é»é¸ä»¥ä¸‹å•é¡Œç¹¼çºŒæå•ï¼š")
    render_suggested_questions(suggested_prompts, auto_submit_prompt)




    # ä¸‹æ–¹è¼¸å…¥æ¡†
    if prompt := st.chat_input("é‡å°æœ¬é¡Œé‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿå¯è©¢å• ESG é¡§å• AI"):
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI å›è¦†ä¸­..."):
                try:
                    reply = call_gpt(prompt)  
                    st.markdown(reply)
                    add_turn(chat_id, prompt, reply)
                except Exception as e:
                    st.error(f"âš ï¸ AI å›è¦†å¤±æ•—ï¼š{str(e)}")

 
else:  # ç¢ºä¿é€™è£¡çš„ else èˆ‡ä¸Šæ–¹ if å°é½Š
    st.success("ğŸ‰ æ‚¨å·²å®Œæˆæœ¬éšæ®µå•å·ï¼")
    baseline = BaselineManager("data/baselines/company_abc.json").get_baseline()
    summary = session.get_summary(company_baseline=baseline)
    summary["user_name"] = st.session_state.get("user_name")
    summary["company_name"] = st.session_state.get("company_name")
    report = ReportManager(summary)
    feedback_mgr = FeedbackManager(summary.get("comparison", []))

    st.markdown("## ğŸ“„ è¨ºæ–·æ‘˜è¦å ±å‘Š")
    st.markdown(f"""
```
{report.generate_text_report()}
```
""")

    st.markdown("## ğŸ’¡ é¡Œç›®å»ºè­°èˆ‡æ”¹å–„æ–¹å‘")
    for fb in feedback_mgr.generate_feedback():
        st.markdown(f"- **{fb['question_id']} å»ºè­°ï¼š** {fb['feedback']}")

    st.markdown("## ğŸ“Œ ç¸½é«”å»ºè­°")
    st.markdown(feedback_mgr.generate_overall_feedback())

    save_to_json(session)
    save_to_sqlite(session)

    if st.session_state.stage == "basic":
        st.divider()
        st.subheader("ğŸš€ æ‚¨å·²å®Œæˆåˆéšè¨ºæ–·ï¼Œæ˜¯å¦é€²å…¥é€²éšå•å·ï¼Ÿ")
        if st.button("ğŸ‘‰ é€²å…¥é€²éšå•å·"):
            st.session_state.stage = "advanced"
            new_qset = load_questions(st.session_state.industry, "advanced", skip_common=True)
            st.session_state.session = AnswerSession(user_id=user_id, question_set=new_qset)
            st.rerun()

# å‘é‡è™•ç†ï¼ˆæš«ä¿ç•™ï¼‰
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")


text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", "ã€‚", ".", "!", "?", "ï¼", "ï¼Ÿ"],
    keep_separator=True
)

base_dir = Path("data/db_pdf_data")

class ChunkMetadata:
    def __init__(self, pdf_path: Path, page: int, section: int, base_dir: Path):
        self.chunk_id = f"{pdf_path.stem}-p{page}-s{section}"
        self.source = pdf_path.name
        self.path = str(pdf_path.parent.relative_to(base_dir))
        self.main_topic = ""
        self.industry = ""
        self.region = ""
        self.page = page
        self.title = ""
        self.language = ""

class VectorStore:
    def __init__(self):
        self.dimension = 1536
        self.index = faiss.IndexFlatIP(self.dimension)
        self.metadata = []

    def add_vectors(self, vectors, metadata_list):
        self.index.add(vectors)
        self.metadata.extend(metadata_list)

    def save(self, output_dir: Path):
        faiss.write_index(self.index, str(output_dir / 'faiss_index.index'))
        with open(output_dir / 'chunk_metadata.json', 'w') as f:
            json.dump(self.metadata, f, indent=2)

# ğŸ“„ app.py

import streamlit as st
from src.loaders.question_loader import load_questions  # æ­£ç¢ºå¼•ç”¨æ­¤å‡½å¼

def show_learning_page():
    # è®€å– session_state ä¸­çš„éšæ®µèˆ‡ç”¢æ¥­è³‡æ–™
    stage = st.session_state.get("stage", "basic")  # é»˜èªç‚ºåˆéš
    industry = st.session_state.get("industry", "é¤é£²æ¥­")  # é»˜èªç‚ºé¤é£²æ¥­

    # æ ¹æ“šéšæ®µè¼‰å…¥é¡Œç›®
    questions = load_questions(industry=industry, stage=stage)

    st.markdown("## ğŸ¯ é–‹å§‹æ‚¨çš„ ESG å­¸ç¿’ä¹‹æ—…")
    st.markdown("""
    æ­¡è¿ä¾†åˆ°å­¸ç¿’é é¢ï¼é€™è£¡æ˜¯ç‚ºæ‚¨é‡èº«è¨­è¨ˆçš„å­¸ç¿’è·¯å¾‘ï¼Œè®“æˆ‘å€‘é–‹å§‹æ¢ç´¢å¦‚ä½•é€²è¡Œç¢³ç›¤æŸ¥ã€äº†è§£ ESG çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸¦å¹«åŠ©æ‚¨èˆ‡ä¼æ¥­åˆä½œæ¸›ç¢³ã€‚
    """)

    # é¡¯ç¤ºå­¸ç¿’å…§å®¹
    st.markdown("### å­¸ç¿’æ¨¡çµ„ 1: ç¢³ç›¤æŸ¥æ¦‚è¿°")
    st.markdown("åœ¨é€™å€‹æ¨¡çµ„ä¸­ï¼Œæ‚¨å°‡å­¸åˆ°å¦‚ä½•é€²è¡Œç¢³ç›¤æŸ¥ã€ä»€éº¼æ˜¯ç¢³è¶³è·¡...")

    # é¡¯ç¤ºé¡Œç›®ï¼Œä¾ç…§ `stage` é¡¯ç¤ºå°æ‡‰é¡Œç›®
    st.markdown("### é€™æ˜¯æ‚¨çš„å•å·é¡Œç›®ï¼š")
    for question in questions:
        st.markdown(f"**{question['text']}**")  # é¡¯ç¤ºå•é¡Œ
        for option in question["options"]:  # é¡¯ç¤ºé¸é …
            st.markdown(f"- {option}")

import streamlit as st

