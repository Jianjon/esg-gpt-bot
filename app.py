import streamlit as st
st.set_page_config(page_title="ESG å•å·è©•æ–·", layout="centered")

# _init_app ç¸½é«”è™•ç†ç’°å¢ƒè¨­å®šèˆ‡ sys.path
import _init_app

# å¥—ç”¨è‡ªå®šç¾© CSS æ¨£å¼
with open("assets/custom_style.css", encoding="utf-8") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# é¡¯ç¤º Topbar
st.markdown("""
    <style>
        div[data-testid="stAppViewContainer"] > .main {{
            padding-top: 64px;
        }}
    </style>
    <div class="topbar">
        <div class="logo">ğŸŒ± ESG Service Path</div>
        <div class="user-info">{}</div>
    </div>
""".format(st.session_state.get("user_name", "æœªç™»å…¥").upper()), unsafe_allow_html=True)

# æª¢æŸ¥æ˜¯å¦é ˆå…ˆé¡¯ç¤º welcome é¡µé¢
if "user_name" not in st.session_state or "industry" not in st.session_state:
    from welcome import show_welcome
    show_welcome()
    st.stop()

from collections import defaultdict
from pathlib import Path
import pandas as pd
import os
import json
import matplotlib.pyplot as plt
import faiss
import fitz
import re
from typing import List, Dict, Tuple

from sessions.answer_session import AnswerSession
from sessions.context_tracker import get_conversation, add_context_entry, add_turn
# from sessions.context_tracker import get_all_summaries
from managers.baseline_manager import BaselineManager
from managers.report_manager import ReportManager
from managers.feedback_manager import FeedbackManager
from session_logger import save_to_json, load_from_json, save_to_sqlite
from vector_builder.pdf_processor import PDFProcessor
from vector_builder.metadata_handler import MetadataHandler
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

MODULE_MAP = {
    "C": "ESG æ•™å­¸å°å…¥ï¼ˆæ•™å­¸å‰å°ï¼‰",
    "B": "é‚Šç•Œè¨­å®šèˆ‡çµ„ç¹”è³‡è¨Š",
    "S": "æ’æ”¾æºè¾¨è­˜èˆ‡ç¢ºèª",
    "D": "æ•¸æ“šæ”¶é›†æ–¹å¼èˆ‡èƒ½åŠ›",
    "M": "å…§éƒ¨ç®¡ç†èˆ‡SOPç¾æ³",
    "R": "å ±å‘Šéœ€æ±‚èˆ‡å¾ŒçºŒè¡Œå‹•"
}

INDUSTRY_FILE_MAP = {
    "é¤é£²æ¥­": "Restaurant.csv",
    "æ—…å®¿æ¥­": "Hotel.csv",
    "é›¶å”®æ¥­": "Retail.csv",
    "å°å‹è£½é€ æ¥­": "SmallManufacturing.csv",
    "ç‰©æµæ¥­": "Logistics.csv",
    "è¾¦å…¬å®¤æœå‹™æ¥­": "Offices.csv"
}

STAGE_MAP = {
    "basic": "beginner",
    "advanced": "intermediate"
}

TOPIC_ORDER = [
    "ESG æ•™å­¸å°å…¥ï¼ˆæ•™å­¸å‰å°ï¼‰",
    "é‚Šç•Œè¨­å®šèˆ‡çµ„ç¹”è³‡è¨Š",
    "æ’æ”¾æºè¾¨è­˜èˆ‡ç¢ºèª",
    "æ•¸æ“šæ”¶é›†æ–¹å¼èˆ‡èƒ½åŠ›",
    "å…§éƒ¨ç®¡ç†èˆ‡SOPç¾æ³",
    "å ±å‘Šéœ€æ±‚èˆ‡å¾ŒçºŒè¡Œå‹•"
]

def load_questions(industry: str, stage: str = "basic", skip_common: bool = False) -> list:
    filename = INDUSTRY_FILE_MAP.get(industry)
    if not filename:
        raise ValueError(f"æ‰¾ä¸åˆ°ç”¢æ¥­å°æ‡‰çš„é¡Œåº«ï¼š{industry}")
    path = os.path.join("data", filename)
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é¡Œåº«æª”æ¡ˆï¼š{path}")

    df = pd.read_csv(path)
    required_cols = ["question_id", "question_text", "difficulty_level", "option_type", "answer_tags"]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"é¡Œåº«ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{col}")

    if stage not in STAGE_MAP:
        raise ValueError(f"è¼¸å…¥å€¼éŒ¯èª¤ï¼šstage æ‡‰ç‚º basic æˆ– advancedï¼Œä½†æ”¶åˆ°ï¼š{stage}")
    mapped_stage = STAGE_MAP[stage]
    df = df[df["difficulty_level"].isin([mapped_stage] if stage == "basic" else ["beginner", "intermediate"])]

    questions = []
    for _, row in df.iterrows():
        qid = row["question_id"]
        topic = row.get("topic_category", "").strip() or MODULE_MAP.get(qid[0], "æœªåˆ†é¡")
        options, option_notes = [], {}
        for opt in ["A", "B", "C", "D", "E"]:
            val = row.get(f"option_{opt}")
            if pd.notna(val):
                options.append(val)
                option_notes[val] = row.get(f"option_{opt}_note", "")

        questions.append({
            "id": qid,
            "industry": row.get("industry_type", industry),
            "text": row.get("question_text", "æœªå¡«é¡Œç›®å…§å®¹"),
            "options": options,
            "option_notes": option_notes,
            "type": row.get("option_type", "single"),
            "topic": topic,
            "difficulty": row.get("difficulty_level", mapped_stage),
            "report_section": row.get("report_section", ""),
            "tags": row.get("answer_tags", "").split("|") if isinstance(row.get("answer_tags"), str) else [],
            "allow_custom_answer": row.get("allow_custom_answer", False),
            "allow_skip": row.get("allow_skip", False),
            "note": row.get("free_answer_note", ""),
            "question_note": row.get("question_note", ""),
            "learning_objective": row.get("learning_objective", ""),
            "report_topic": row.get("report_topic", ""),
            "learning_goal": row.get("learning_goal", ""),
            "follow_up": row.get("follow_up", "")
        })

    if skip_common:
        questions = [q for q in questions if not (q["id"].startswith("C0"))]

    def question_sort_key(q):
        topic_index = TOPIC_ORDER.index(q["topic"]) if q["topic"] in TOPIC_ORDER else 999
        qid = q["id"]
        num_part = int(qid[1:]) if qid[1:].isdigit() else 999
        return (topic_index, num_part)

    return sorted(questions, key=question_sort_key)

# ===== åˆå§‹åŒ–è¨­å®š =====
if "stage" not in st.session_state:
    st.session_state.stage = "basic"
if "jump_to" not in st.session_state:
    st.session_state["jump_to"] = None

user_id = f"{st.session_state.get('company_name', 'unknown')}_{st.session_state.get('user_name', 'user')}"
current_stage = st.session_state.stage

questions = load_questions(
    industry=st.session_state.industry,
    stage=current_stage,
    skip_common=True
)

if "session" not in st.session_state:
    session_file = Path("data/sessions") / f"{st.session_state.company_name}_{st.session_state.user_name}.json"
    if session_file.exists():
        if st.session_state.get("reset_data"):
            session_file.unlink()
            st.toast("âœ… å·²æ¸…é™¤åŸæœ‰ç´€éŒ„ï¼Œå•å·å°‡å¾é ­é–‹å§‹ã€‚")
            st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)
        else:
            session = load_from_json(user_id, questions)
            if session:
                if st.button("ğŸ”„ ç¹¼çºŒä¸Šæ¬¡ç­”é¡Œé€²åº¦"):
                    st.session_state.session = session
                    st.rerun()
                else:
                    st.warning("åµæ¸¬åˆ°æ‚¨æœ‰æœªå®Œæˆçš„å•å·ç´€éŒ„ï¼Œæ‚¨å¯ä»¥é¸æ“‡ç¹¼çºŒæˆ–é‡æ–°é–‹å§‹ã€‚")
                    st.stop()
    else:
        st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)

session = st.session_state.session
current_q = session.get_current_question()

# ===== å•Ÿç”¨ jump_to è·³é¡Œ =====
if st.session_state.get("jump_to"):
    qid = st.session_state["jump_to"]
    index = next((i for i, item in enumerate(session.question_set) if item["id"] == qid), None)
    if index is not None:
        session.jump_to(index)
        st.session_state["jump_to"] = None
        st.rerun()

# ===== å´é‚Šåˆ—é‡æ§‹ï¼ˆä½¿ç”¨ question_setï¼‰ =====
with st.sidebar:
    st.title("ğŸ“‹ ESG æ™ºèƒ½å•å·è¨ºæ–¶ | æ·¨é›¶å°å¹«æ‰‹")
    st.markdown("---")
    st.header("ğŸ‘¤ ä½¿ç”¨è€…è³‡è¨Š")
    st.markdown(f"**å§“åï¼š** {st.session_state.user_name}")
    st.markdown(f"**éšæ®µï¼š** {'åˆéš' if st.session_state.stage == 'basic' else 'é€²éš'}")
    st.markdown(f"**ç›®å‰é€²åº¦ï¼š** {session.current_index + 1} / {len(session.question_set)}")
    st.markdown("---")
    st.markdown("### ğŸ“Š ä¸»é¡Œé€²åº¦æ¦‚è¦½")

    current_topic = current_q.get("topic") if current_q else None
    answered_ids = {r["question_id"] for r in session.responses}

    questions_by_topic = defaultdict(list)
    for q in session.question_set:
        topic = q.get("topic", "æœªåˆ†é¡")
        questions_by_topic[topic].append(q)

    for topic, q_list in questions_by_topic.items():
        total = len(q_list)
        answered = sum(1 for q in q_list if q["id"] in answered_ids)
        checked = "âœ… " if answered == total else ""
        expanded = topic == current_topic

        with st.expander(f"{checked}{topic}", expanded=expanded):
            for idx, q in enumerate(q_list, 1):
                is_done = q["id"] in answered_ids
                label = f"{idx}. {q.get('text', '')[:15]}..."
                if is_done:
                    label += " âœ”"

                key = f"jump_to_{q['id']}"
                if st.button(label, key=key):
                    st.session_state["jump_to"] = q["id"]
                    st.rerun()

# å›ºå®šä¸»é«”å®¹å™¨
st.markdown('<div class="main-content-container">', unsafe_allow_html=True)

# ä¸»é«”å…§å®¹ï¼šå•é¡Œä¸»é«”ã€èªªæ˜èˆ‡é¸é …
if current_q:
    st.markdown(f"#### ğŸ¯ å­¸ç¿’ä¸»é¡Œï¼š<br>{current_q.get('learning_goal', '')}", unsafe_allow_html=True)
    st.markdown("---")
    st.markdown(f"<h3>{current_q.get('text', '')}</h3>", unsafe_allow_html=True)
    st.markdown(f"**é¡Œç›®èªªæ˜ï¼š** {current_q.get('question_note', '')}")

    # é¡¯ç¤ºé¸é …å€‘
    options = current_q["options"]
    option_notes = current_q.get("option_notes", {})
    labeled_options = [f"{opt}ï¼š{option_notes.get(opt, '')}" for opt in options]

    selected = []

    if current_q["type"] == "single":
        selected_option = st.radio("å¯é¸æ“‡ï¼š", labeled_options)
        if selected_option:
            selected = [selected_option.split("ï¼š")[0]]
    else:
        st.markdown("å¯è¤‡é¸ï¼š")
        for opt in labeled_options:
            opt_key = opt.split("ï¼š")[0]
            if st.checkbox(opt, key=opt_key):
                selected.append(opt_key)

    st.markdown("---")
    # å°èˆªæŒ‰éˆ•å€å¡Š
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("ğŸ‘ˆ ä¸Šä¸€é¡Œ", key="btn_prev", use_container_width=True):
            session.go_back()
            st.rerun()
    with col2:
        if st.button("ğŸ‘‰ ä¸‹ä¸€é¡Œ", key="btn_next", use_container_width=True):
            # æ ¹æ“šé¡Œå‹è™•ç†ç­”æ¡ˆæ ¼å¼
            if current_q["type"] == "single":
                answer_payload = selected[0] if selected else ""
            else:
                answer_payload = selected

            # æäº¤ç­”æ¡ˆ
            result = session.submit_response(answer_payload)
            add_context_entry(current_q["id"], selected, current_q["text"])
            save_to_json(session)
            if "error" in result:
                st.error(result["error"])
            else:
                st.rerun()

    # === GPT å°è©±å¼å•ç­”å€å¡Š ===
    st.divider()
    st.markdown("#### ğŸ¤– å•é¡Œæ©Ÿå™¨äººï¼ˆé‡å°æœ¬é¡Œé€²è¡Œå»¶ä¼¸æå•ï¼‰")

    # åˆå§‹åŒ–å°è©±è¨˜æ†¶
    if "qa_threads" not in st.session_state:
        st.session_state.qa_threads = {}

    chat_id = current_q["id"]
    from sessions.context_tracker import get_conversation, add_turn
    from src.utils.gpt_tools import call_gpt

    # é¡¯ç¤ºæ­·å²å°è©±
    for msg in get_conversation(chat_id):
        with st.chat_message("user"):
            st.markdown(msg["user"])
        with st.chat_message("assistant"):
            st.markdown(msg["gpt"])

    # æå•å»ºè­°
    st.markdown("##### ğŸ’¡ æå•å»ºè­°")
    st.info(current_q.get("follow_up", "ç›®å‰å°šç„¡æç¤ºï¼Œæ‚¨å¯è‡ªç”±ç™¼å•"))

    # ä¸‹æ–¹è¼¸å…¥æ¡†
    if prompt := st.chat_input("é‡å°æœ¬é¡Œé‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿå¯è©¢å• ESG é¡§å• AI"):
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI å›è¦†ä¸­..."):
                try:
                    reply = call_gpt(prompt, current_q["text"], current_q.get("learning_goal", ""))
                    st.markdown(reply)
                    add_turn(chat_id, prompt, reply)
                except Exception as e:
                    st.error(f"âš ï¸ AI å›è¦†å¤±æ•—ï¼š{str(e)}")

 
else:  # ç¢ºä¿é€™è£¡çš„ else èˆ‡ä¸Šæ–¹ if å°é½Š
    st.success("ğŸ‰ æ‚¨å·²å®Œæˆæœ¬éšæ®µå•å·ï¼")
    baseline = BaselineManager("data/baselines/company_abc.json").get_baseline()
    summary = session.get_summary(company_baseline=baseline)
    summary["user_name"] = st.session_state.get("user_name")
    summary["company_name"] = st.session_state.get("company_name")
    report = ReportManager(summary)
    feedback_mgr = FeedbackManager(summary.get("comparison", []))

    st.markdown("## ğŸ“„ è¨ºæ–·æ‘˜è¦å ±å‘Š")
    st.markdown(f"""
```
{report.generate_text_report()}
```
""")

    st.markdown("## ğŸ’¡ é¡Œç›®å»ºè­°èˆ‡æ”¹å–„æ–¹å‘")
    for fb in feedback_mgr.generate_feedback():
        st.markdown(f"- **{fb['question_id']} å»ºè­°ï¼š** {fb['feedback']}")

    st.markdown("## ğŸ“Œ ç¸½é«”å»ºè­°")
    st.markdown(feedback_mgr.generate_overall_feedback())

    save_to_json(session)
    save_to_sqlite(session)

    if st.session_state.stage == "basic":
        st.divider()
        st.subheader("ğŸš€ æ‚¨å·²å®Œæˆåˆéšè¨ºæ–·ï¼Œæ˜¯å¦é€²å…¥é€²éšå•å·ï¼Ÿ")
        if st.button("ğŸ‘‰ é€²å…¥é€²éšå•å·"):
            st.session_state.stage = "advanced"
            new_qset = load_questions(st.session_state.industry, "advanced", skip_common=True)
            st.session_state.session = AnswerSession(user_id=user_id, question_set=new_qset)
            st.rerun()

# å‘é‡è™•ç†ï¼ˆæš«ä¿ç•™ï¼‰
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    dimensions=1536
)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", "ã€‚", ".", "!", "?", "ï¼", "ï¼Ÿ"],
    keep_separator=True
)

base_dir = Path("data/db_pdf_data")

class ChunkMetadata:
    def __init__(self, pdf_path: Path, page: int, section: int, base_dir: Path):
        self.chunk_id = f"{pdf_path.stem}-p{page}-s{section}"
        self.source = pdf_path.name
        self.path = str(pdf_path.parent.relative_to(base_dir))
        self.main_topic = ""
        self.industry = ""
        self.region = ""
        self.page = page
        self.title = ""
        self.language = ""

class VectorStore:
    def __init__(self):
        self.dimension = 1536
        self.index = faiss.IndexFlatIP(self.dimension)
        self.metadata = []

    def add_vectors(self, vectors, metadata_list):
        self.index.add(vectors)
        self.metadata.extend(metadata_list)

    def save(self, output_dir: Path):
        faiss.write_index(self.index, str(output_dir / 'faiss_index.index'))
        with open(output_dir / 'chunk_metadata.json', 'w') as f:
            json.dump(self.metadata, f, indent=2)

