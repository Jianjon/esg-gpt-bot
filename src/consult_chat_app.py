import streamlit as st
from src.loaders.question_loader import load_questions
from src.sessions.answer_session import AnswerSession
from src.sessions import context_tracker
from src.generators.report_generator import generate_basic_report
from src.utils.vector_guard import VectorStore
from src.managers.guided_rag import GuidedRAG
from src.managers.gpt_rewrite import rewrite_question_to_conversational
from src.utils.topic_progress import get_topic_progress
from session_logger import save_to_json
import openai
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

from src.utils.gpt_tools import call_gpt

def call_gpt(prompt: str, model: str = "gpt-3.5-turbo") -> str:
    """
    å‘¼å« GPT æ¨¡å‹ï¼Œæ ¹æ“šæä¾›çš„ prompt ç”Ÿæˆå›æ‡‰ã€‚
    Params:
        - prompt: æä¾›çµ¦ GPT çš„æ–‡å­—æç¤º
        - model: ä½¿ç”¨çš„ GPT æ¨¡å‹ï¼ˆé è¨­ç‚º gpt-3.5-turboï¼‰
    Return:
        - GPT å›æ‡‰çš„æ–‡å­—
    """
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ ESG é¡§å•ï¼Œè«‹ç”¨ç°¡æ½”ä¸”å¯¦ç”¨çš„æ–¹å¼å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=500
        )
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âš ï¸ GPT å›æ‡‰éŒ¯èª¤ï¼š{e}")
        return "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•å–å¾—å›è¦†ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# è¼‰å…¥è‡ªè¨‚æ¨£å¼
def local_css(file_path):
    with open(file_path, encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

local_css("assets/custom_style.css")

# --- Sidebarï¼šChatGPT é¢¨æ ¼çš„ ESG Service Path ---
st.sidebar.markdown("## ESG Service Path")
st.sidebar.markdown("---")

# é¡¯ç¤ºä½¿ç”¨è€…åç¨±
user_name = st.session_state.get("user_name", "Guest")
st.sidebar.markdown(f"ä½¿ç”¨è€…ï¼š**{user_name}**")

# åˆå§‹åŒ–å•å·ç‹€æ…‹
if 'welcome_submitted' not in st.session_state or not st.session_state['welcome_submitted']:
    st.warning("è«‹å…ˆå®Œæˆç”¢æ¥­é¸æ“‡èˆ‡å…¬å¸åŸºæœ¬è³‡æ–™å¡«å¯«ã€‚")
    st.stop()

industry = st.session_state['industry']
stage = st.session_state.get('stage', 'basic')

# åˆå§‹åŒ– AnswerSession
user_id = f"{st.session_state.get('company_name', 'unknown')}_{st.session_state.get('user_name', 'user')}"
questions = load_questions(industry, stage)

if 'session' not in st.session_state:
    st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)

session = st.session_state.session

# é¡¯ç¤ºä¸»é¡Œé€²åº¦åœ–
answered_ids = {r["question_id"] for r in session.responses}
fig = get_topic_progress(session.question_set, answered_ids)
st.sidebar.pyplot(fig)

# é¡¯ç¤ºç›®å‰é¡Œç›®
current_q = session.get_current_question()
if current_q:
    # ä½¿ç”¨ GPT é‡å¯«é¡Œç›®ç‚ºå°è©±é¢¨æ ¼
    gpt_text = rewrite_question_to_conversational(
        current_q["text"],
        question_type=current_q.get("type", "single"),
        learning_goal=current_q.get("learning_goal", ""),
        use_gpt=True
    )

    # é¡¯ç¤ºé¡Œç›®èˆ‡ GPT é‡å¯«çš„å°è©±é¢¨æ ¼é¡Œç›®
    st.markdown(
        f"<div class='fixed-question'>{current_q['text']}</div>",
        unsafe_allow_html=True
    )
    st.markdown(f"<div class='ai-message'>{gpt_text}</div>", unsafe_allow_html=True)

    # ä½¿ç”¨è€…è¼¸å…¥æ¡†
    user_input = st.text_input("ä½ çš„å›è¦†ï¼š", key=f"input_{session.current_index}")
    if user_input:
        st.markdown(f"<div class='user-message'>{user_input}</div>", unsafe_allow_html=True)
        session.save_answer(user_input)

        # è¨˜éŒ„å°è©±åˆ° context_tracker
        context_tracker.add_turn(current_q["id"], user_input, "ï¼ˆAI å›è¦†å¾…ç”Ÿæˆï¼‰")

        # è‡ªå‹•å„²å­˜é€²åº¦
        save_to_json(session)

        # ä¸‹ä¸€é¡ŒæŒ‰éˆ•
        if st.button("ğŸ‘‰ ä¸‹ä¸€é¡Œ"):
            # è‡ªå‹•æ‘˜è¦
            summary = context_tracker.update(current_q["id"], user_input)

            # ä¸‹ä¸€é¡Œ
            session.advance()
            st.experimental_rerun()
else:
    st.success("å•å·å·²å®Œæˆï¼è«‹é»é¸ä¸‹æ–¹æŒ‰éˆ•ç”¢å‡ºè¨ºæ–·æ‘˜è¦å ±å‘Šã€‚")
    if st.button("ç”¢å‡ºå ±å‘Š"):
        report = generate_basic_report(session.get_answers(), context_tracker.get_all_summaries())
        st.text_area("è¨ºæ–·å ±å‘Šï¼š", report, height=400)

# ====== å¼•å°å¼è‡ªç”±å•ç­”ï¼ˆé€²éšæ”¯æ´ï¼‰======
st.markdown("---")
st.subheader("éœ€è¦å¹«åŠ©å—ï¼Ÿèˆ‡é¡§å•èŠèŠï¼š")

from pathlib import Path
vector_path = Path("data/vector_output")
guided_rag = GuidedRAG(vector_path=vector_path)


if 'guided_chat' not in st.session_state:
    st.session_state['guided_chat'] = []
if 'guided_turns' not in st.session_state:
    st.session_state['guided_turns'] = 0

user_question = st.text_input("è¼¸å…¥ä½ å°æ­¤é¡Œçš„ç–‘å•ï¼Œæˆ–ç›´æ¥è¼¸å…¥ã€ä¸çŸ¥é“ã€ï¼š", key=f"help_{session.current_index}")

if user_question:
    st.session_state['guided_turns'] += 1
    st.session_state['guided_chat'].append(("user", user_question))

    ai_reply, related_chunks = guided_rag.ask(
        user_question,
        history=st.session_state['guided_chat'],
        turn=st.session_state['guided_turns']
    )

    st.markdown(f"<div class='ai-message'>{ai_reply}</div>", unsafe_allow_html=True)
    st.session_state['guided_chat'].append(("assistant", ai_reply))

    # è¨˜éŒ„å°è©±åˆ° context_tracker
    context_tracker.add_turn(current_q["id"], user_question, ai_reply)

    with st.expander("åƒè€ƒè³‡æ–™æ®µè½"):
        for chunk in related_chunks:
            st.markdown(f"**{chunk['source']}** - ç¬¬ {chunk['page']} é ")
            st.markdown(f"> {chunk['text'][:300]}...\n")

    if st.session_state['guided_turns'] >= 3:
        st.info("é€™é¡Œè‹¥ä»ç„¡æ³•åˆ¤æ–·ï¼Œæˆ‘å€‘å¯ä»¥å…ˆé€²å…¥ä¸‹ä¸€é¡Œã€‚")
        if st.button("è·³éæ­¤é¡Œ"):
            session.save_answer("ï¼ˆæœªå›ç­”ï¼‰")
            context_tracker.update(current_q['id'], "æœªå›ç­”ï¼ˆç”±å¼•å°ç³»çµ±è¨˜éŒ„ï¼‰")
            session.advance()
            st.session_state['guided_turns'] = 0
            st.session_state['guided_chat'] = []
            st.experimental_rerun()
