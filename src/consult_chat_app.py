import streamlit as st
from src.loaders.question_loader import load_questions
from src.sessions.answer_session import AnswerSession
from src.sessions import context_tracker
from utils.report_generator import generate_basic_report
from src.managers.guided_rag import GuidedRAG
from src.managers.gpt_rewrite import rewrite_question_to_conversational
from src.utils.topic_progress import get_topic_progress
from utils.session_logger import save_to_json
from vector_builder.vector_store import VectorStore
import openai
import os
from dotenv import load_dotenv
from src.components.suggest_box import render_suggested_questions


# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

from src.utils.gpt_tools import call_gpt


# è¼‰å…¥è‡ªè¨‚æ¨£å¼
def local_css(file_path):
    with open(file_path, encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

local_css("assets/custom_style.css")

# --- Sidebarï¼šChatGPT é¢¨æ ¼çš„ ESG Service Path ---
st.sidebar.markdown("## ESG Service Pathï¼šæ·¨é›¶GPT")
st.sidebar.markdown("---")

# é¡¯ç¤ºä½¿ç”¨è€…åç¨±
user_name = st.session_state.get("user_name", "Guest")
st.sidebar.markdown(f"ä½¿ç”¨è€…ï¼š**{user_name}**")

# åˆå§‹åŒ–å•å·ç‹€æ…‹
if 'welcome_submitted' not in st.session_state or not st.session_state['welcome_submitted']:
    st.warning("è«‹å…ˆå®Œæˆç”¢æ¥­é¸æ“‡èˆ‡å…¬å¸åŸºæœ¬è³‡æ–™å¡«å¯«ã€‚")
    st.stop()

industry = st.session_state['industry']
stage = st.session_state.get('stage', 'basic')

# åˆå§‹åŒ– AnswerSession
user_id = f"{st.session_state.get('company_name', 'unknown')}_{st.session_state.get('user_name', 'user')}"
questions = load_questions(industry, stage)

if 'session' not in st.session_state:
    st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)

session = st.session_state.session

# é¡¯ç¤ºä¸»é¡Œé€²åº¦åœ–
answered_ids = {r["question_id"] for r in session.responses}
fig = get_topic_progress(session.question_set, answered_ids)
st.sidebar.pyplot(fig)

# é¡¯ç¤ºç›®å‰é¡Œç›®
current_q = session.get_current_question()
if current_q:
    # ä½¿ç”¨ GPT é‡å¯«é¡Œç›®ç‚ºå°è©±é¢¨æ ¼
    gpt_text = rewrite_question_to_conversational(
        current_q["text"],
        question_type=current_q.get("type", "single"),
        learning_goal=current_q.get("learning_goal", ""),
        use_gpt=True
    )

    # é¡¯ç¤ºé¡Œç›®èˆ‡ GPT é‡å¯«çš„å°è©±é¢¨æ ¼é¡Œç›®
    st.markdown(
        f"<div class='fixed-question'>{current_q['text']}</div>",
        unsafe_allow_html=True
    )
    st.markdown(f"<div class='ai-message'>{gpt_text}</div>", unsafe_allow_html=True)

    # === GPT å°è©±å¼å•ç­”å€å¡Š ===
    st.divider()
    st.markdown("### ğŸ¤– å•é¡Œæ©Ÿå™¨äººï¼ˆé‡å°æœ¬é¡Œé€²è¡Œå»¶ä¼¸æå•ï¼‰")

    # åˆå§‹åŒ– context_tracker çš„ chat è¨˜æ†¶
    if "qa_threads" not in st.session_state:
        st.session_state["qa_threads"] = {}

    from sessions.context_tracker import get_conversation, add_turn
    from src.utils.gpt_tools import call_gpt

    chat_id = current_q["id"]
    history = get_conversation(chat_id)

    # å®šç¾©é»æŒ‰å»ºè­°é¡Œç›®å¾Œçš„é€å‡ºè™•ç†
    def auto_submit_prompt(prompt):
        st.session_state.chat_input = prompt
        st.rerun()

    # é¡¯ç¤ºå»ºè­°æå•ï¼ˆä½¿ç”¨ follow_up è‡ªå‹•åˆ†å‰²ï¼‰
    suggested_prompts = current_q.get("follow_up", "")
    if suggested_prompts:
        render_suggested_questions(suggested_prompts.split("|"), auto_submit_prompt)

    # é¡¯ç¤ºæ­·å²å°è©±ç´€éŒ„ï¼ˆä¸ŠåŠéƒ¨ï¼‰
    if history:
        st.markdown("##### ğŸ“œ æ­·å²å°è©±ç´€éŒ„")
        for msg in history:
            with st.chat_message("user"):
                st.markdown(msg["user"])
            with st.chat_message("assistant"):
                st.markdown(msg["gpt"])

    # è¨­å®šæ¯é¡Œæœ€å¤šå°è©±è¼ªæ•¸
    MAX_TURNS = 5
    if len(history) >= MAX_TURNS:
        st.warning("æ‚¨å·²é‡å°æœ¬é¡Œé€²è¡Œäº†å¤šè¼ªæå•ï¼Œå»ºè­°å‰å¾€ä¸‹ä¸€é¡Œä»¥æŒçºŒå­¸ç¿’ ğŸ˜Š")
        if st.button("ğŸ‘‰ å‰å¾€ä¸‹ä¸€é¡Œ"):
            session.next()
            st.rerun()
        st.stop()

    # ä¸‹åŠéƒ¨è¼¸å…¥å€ï¼ˆst.chat_inputï¼‰
    if prompt := st.chat_input("é‡å°æœ¬é¡Œé‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿå¯è©¢å•æ·¨é›¶AIé¡§å•"):
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI å›è¦†ä¸­..."):
                try:
                    gpt_reply = call_gpt(
                        prompt=prompt,
                        question_text=current_q["text"],
                        learning_goal=current_q.get("learning_goal", ""),
                        chat_history=get_conversation(chat_id),
                        industry=st.session_state.get("industry", "")
                    )
                    st.markdown(gpt_reply)
                    add_turn(chat_id, prompt, gpt_reply)

                except Exception as e:
                    st.error(f"âš ï¸ AI å›è¦†å¤±æ•—ï¼š{str(e)}")