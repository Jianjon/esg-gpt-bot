import streamlit as st
from src.loaders.question_loader import load_questions
from src.sessions.answer_session import AnswerSession
from src.sessions import context_tracker
from src.generators.report_generator import generate_basic_report
from src.utils.vector_guard import VectorStore
from src.managers.guided_rag import GuidedRAG
from src.managers.gpt_rewrite import rewrite_question_to_conversational
from src.utils.topic_progress import get_topic_progress
from session_logger import save_to_json
import openai
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

from src.utils.gpt_tools import call_gpt

def call_gpt(prompt: str, model: str = "gpt-3.5-turbo") -> str:
    """
    å‘¼å« GPT æ¨¡å‹ï¼Œæ ¹æ“šæä¾›çš„ prompt ç”Ÿæˆå›æ‡‰ã€‚
    Params:
        - prompt: æä¾›çµ¦ GPT çš„æ–‡å­—æç¤º
        - model: ä½¿ç”¨çš„ GPT æ¨¡å‹ï¼ˆé è¨­ç‚º gpt-3.5-turboï¼‰
    Return:
        - GPT å›æ‡‰çš„æ–‡å­—
    """
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ ESG é¡§å•ï¼Œè«‹ç”¨ç°¡æ½”ä¸”å¯¦ç”¨çš„æ–¹å¼å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=500
        )
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âš ï¸ GPT å›æ‡‰éŒ¯èª¤ï¼š{e}")
        return "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•å–å¾—å›è¦†ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# è¼‰å…¥è‡ªè¨‚æ¨£å¼
def local_css(file_path):
    with open(file_path, encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

local_css("assets/custom_style.css")

# --- Sidebarï¼šChatGPT é¢¨æ ¼çš„ ESG Service Path ---
st.sidebar.markdown("## ESG Service Path")
st.sidebar.markdown("---")

# é¡¯ç¤ºä½¿ç”¨è€…åç¨±
user_name = st.session_state.get("user_name", "Guest")
st.sidebar.markdown(f"ä½¿ç”¨è€…ï¼š**{user_name}**")

# åˆå§‹åŒ–å•å·ç‹€æ…‹
if 'welcome_submitted' not in st.session_state or not st.session_state['welcome_submitted']:
    st.warning("è«‹å…ˆå®Œæˆç”¢æ¥­é¸æ“‡èˆ‡å…¬å¸åŸºæœ¬è³‡æ–™å¡«å¯«ã€‚")
    st.stop()

industry = st.session_state['industry']
stage = st.session_state.get('stage', 'basic')

# åˆå§‹åŒ– AnswerSession
user_id = f"{st.session_state.get('company_name', 'unknown')}_{st.session_state.get('user_name', 'user')}"
questions = load_questions(industry, stage)

if 'session' not in st.session_state:
    st.session_state.session = AnswerSession(user_id=user_id, question_set=questions)

session = st.session_state.session

# é¡¯ç¤ºä¸»é¡Œé€²åº¦åœ–
answered_ids = {r["question_id"] for r in session.responses}
fig = get_topic_progress(session.question_set, answered_ids)
st.sidebar.pyplot(fig)

# é¡¯ç¤ºç›®å‰é¡Œç›®
current_q = session.get_current_question()
if current_q:
    # ä½¿ç”¨ GPT é‡å¯«é¡Œç›®ç‚ºå°è©±é¢¨æ ¼
    gpt_text = rewrite_question_to_conversational(
        current_q["text"],
        question_type=current_q.get("type", "single"),
        learning_goal=current_q.get("learning_goal", ""),
        use_gpt=True
    )

    # é¡¯ç¤ºé¡Œç›®èˆ‡ GPT é‡å¯«çš„å°è©±é¢¨æ ¼é¡Œç›®
    st.markdown(
        f"<div class='fixed-question'>{current_q['text']}</div>",
        unsafe_allow_html=True
    )
    st.markdown(f"<div class='ai-message'>{gpt_text}</div>", unsafe_allow_html=True)

    # === GPT å°è©±å¼å•ç­”å€å¡Š ===
    st.divider()
    st.markdown("### ğŸ¤– å•é¡Œæ©Ÿå™¨äººï¼ˆé‡å°æœ¬é¡Œé€²è¡Œå»¶ä¼¸æå•ï¼‰")

    # åˆå§‹åŒ– context_tracker çš„ chat è¨˜æ†¶
    if "qa_threads" not in st.session_state:
        st.session_state.qa_threads = {}

    from sessions.context_tracker import get_conversation, add_turn
    from src.utils.gpt_tools import call_gpt  # ä½ åŸæœ¬è‡ªå®šç¾©çš„ GPT å‘¼å«å‡½å¼

    chat_id = current_q["id"]
    history = get_conversation(chat_id)

    # é¡¯ç¤ºå°è©±ç´€éŒ„ï¼ˆä¸ŠåŠéƒ¨ï¼‰
    for msg in history:
        with st.chat_message("user"):
            st.markdown(msg["user"])
        with st.chat_message("assistant"):
            st.markdown(msg["gpt"])

    # åŠ ä¸Š follow-up æç¤ºï¼ˆå»ºè­°æå•æ–¹å‘ï¼‰
    st.markdown("##### ğŸ’¡ æå•å»ºè­°")
    st.info(current_q.get("follow_up", "ç›®å‰å°šç„¡æç¤ºï¼Œæ‚¨å¯è‡ªç”±ç™¼å•"))

    # ä¸‹åŠéƒ¨è¼¸å…¥å€
    if prompt := st.chat_input("é‡å°æœ¬é¡Œé‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿå¯è©¢å• ESG å°ˆå®¶ AI"):
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("AI å›è¦†ä¸­..."):
                try:
                    gpt_reply = call_gpt(prompt, current_q["text"], current_q.get("learning_goal", ""))
                    st.markdown(gpt_reply)
                    add_turn(chat_id, prompt, gpt_reply)
                except Exception as e:
                    st.error(f"âš ï¸ AI å›è¦†å¤±æ•—ï¼š{str(e)}")

