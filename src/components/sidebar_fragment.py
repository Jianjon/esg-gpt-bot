# src/components/sidebar_fragment.py

import streamlit as st
from collections import defaultdict
from typing import List, Dict
from src.utils.report_utils import generate_full_gpt_report
from src.managers.profile_manager import get_user_profile

user_profile = get_user_profile()

@st.fragment
def render_sidebar_fragment(session, current_q, key=None):  # â† å¤šé€™å€‹ key=None
    st.title("ğŸ“‹ ESG Service Pathï¼šæ·¨é›¶GPT")
    st.markdown("---")

    # === ä½¿ç”¨è€…è³‡è¨Š ===
    st.header("ğŸ‘¤ ä½¿ç”¨è€…è³‡è¨Š")
    st.markdown(f"**å§“åï¼š** {st.session_state.get('user_name', 'æœªç™»å…¥')}")
    st.markdown(f"**éšæ®µï¼š** {'åˆéš' if st.session_state.get('stage') == 'basic' else 'é€²éš'}")

    # === å•å·é€²åº¦çµ±è¨ˆï¼ˆæŠ½æˆå‡½å¼ï¼‰ ===
    answered = render_question_progress(session)

    st.markdown("---")
    st.markdown("### ğŸ“Š ä¸»é¡Œé€²åº¦æ¦‚è¦½")

    # === ä¸»é¡Œåˆ†é¡èˆ‡å‹¾é¸é€²åº¦ ===
    current_topic = current_q.get("topic", "")
    topic_groups = defaultdict(list)
    for q in session.question_set:
        topic_groups[q["topic"]].append(q)

    if "jump_to" not in st.session_state:
        st.session_state["jump_to"] = None

    for topic, q_list in topic_groups.items():
        total = len(q_list)
        answered_ids = {r["question_id"] for r in session.responses}
        topic_answered = sum(1 for q in q_list if q["id"] in answered_ids)
        checked = "âœ… " if topic_answered == total else ""
        expanded = topic == current_topic

        with st.expander(f"{checked}{topic}", expanded=expanded):
            for i, q in enumerate(q_list):
                qid = q["id"]
                is_answered = qid in answered_ids
                display_text = q.get("text", "").strip().replace("\n", " ")
                short_label = display_text[:16] + "..." if len(display_text) > 16 else display_text
                prefix = "âœ”ï¸" if is_answered else "â–«ï¸"
                label = f"{prefix} {i + 1}. {short_label}"

                if st.button(label, key=f"jump_to_{qid}", use_container_width=True):
                    st.session_state["jump_to"] = qid
                    st.session_state["_trigger_all_sections"] = st.session_state.get("_trigger_all_sections", 0) + 1
                    st.rerun()

    # === å ±å‘Šç”¢å‡ºå€å¡Š ===
    st.markdown("---")
    st.subheader("ğŸ§¾ æ¸¬é©—è¨ºæ–·å ±å‘Š")

    REQUIRED_QUESTIONS = 10
    if answered < REQUIRED_QUESTIONS:
        st.info(
            f"âš ï¸ è«‹å…ˆå®Œæˆè‡³å°‘ {REQUIRED_QUESTIONS} é¡Œï¼Œæ‰èƒ½æŸ¥çœ‹è¨ºæ–·å ±å‘Šã€‚\n\n"
            "ğŸ’¡ å»ºè­°å®Œæ•´åƒèˆ‡ AI é¡§å•æ•™å­¸äº’å‹•ï¼Œè®“å ±å‘Šæ›´è²¼è¿‘æ‚¨çš„æƒ…å¢ƒèˆ‡éœ€æ±‚ã€‚"
        )
    else:
        if st.button("ğŸ“„ ç”¢å‡ºç›®å‰å ±å‘Šï¼ˆGPT-4ï¼‰", use_container_width=True):
            context_history = st.session_state.get("context_history", [])
            if not context_history:
                st.warning("âš ï¸ å°šæœªæœ‰ä»»ä½• AI å›é¥‹ç´€éŒ„ï¼Œè«‹å…ˆå®Œæˆè‡³å°‘ä¸€é¡Œäº’å‹•å¾Œå†ç”¢å‡ºå ±å‘Šã€‚")
            else:
                with st.spinner("AI é¡§å•åˆ†æä¸­..."):
                    try:
                        tone = st.session_state.get("preferred_tone", "professional")
                        report = generate_full_gpt_report(user_profile, context_history, tone=tone)
                        st.success("âœ… å ±å‘Šç”¢å‡ºå®Œæˆï¼")
                        st.markdown(report)
                    except Exception as e:
                        st.error(f"âŒ å ±å‘Šç”¢å‡ºå¤±æ•—ï¼š{e}")

# === æŠ½å‡ºçš„é€²åº¦å‡½å¼ ===
def render_question_progress(session) -> int:
    answered_ids = {r["question_id"] for r in session.responses}
    total_questions = len(session.question_set)
    answered = len(answered_ids)

    st.markdown(f"**ç›®å‰é€²åº¦ï¼š** {answered} / {total_questions} é¡Œ")
    st.progress(answered / total_questions if total_questions else 0)

    # Debug å¯é¸ï¼šé¡¯ç¤ºå·²ç­”é¡Œ ID
    st.caption(f"ğŸ§ª å·²ä½œç­”é¡Œç›® IDï¼š{list(answered_ids)}")
    return answered
